

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.jpg">
  <link rel="icon" href="/img/icon.jpg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="rainstar">
  <meta name="keywords" content="">
  
    <meta name="description" content="[TOC] 机器学习算法类型  监督学习（常用） 非监督学习  监督学习 监督学习算法通过学习输入（x）到输出（y）的映射关系。在监督学习中，输入算法的为x，y（带标签）。 主要类型有回归和分类： 回归（regression） 回归是监督学习的一种类型，任务是预测一个连续的数值。例如，根据房屋面积预测房价。在这个问题中，我们可以使用不同的算法来拟合数据（例如线性回归或曲线拟合）">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记">
<meta property="og:url" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="RainStar&#39;s Blog">
<meta property="og:description" content="[TOC] 机器学习算法类型  监督学习（常用） 非监督学习  监督学习 监督学习算法通过学习输入（x）到输出（y）的映射关系。在监督学习中，输入算法的为x，y（带标签）。 主要类型有回归和分类： 回归（regression） 回归是监督学习的一种类型，任务是预测一个连续的数值。例如，根据房屋面积预测房价。在这个问题中，我们可以使用不同的算法来拟合数据（例如线性回归或曲线拟合）">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746688424952.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746696598520.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746697450500.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746699765119.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746772964358.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746773968352.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746774767762.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746781317379.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746783342411.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746782075789.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747034769708.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747037413940.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747037483049.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747038198520.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747038691415.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747041118033.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747041904302.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747042064277.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747042683490.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747042730321.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747042782174.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747042823630.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747121658737.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747122250668.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747123119951.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747126935553.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747127662190.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747127561170.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747128421745.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747128554959.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747128606609.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747207959425.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747208024315.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747208048886.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747208097956.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747208465141.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747210389610.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747211824874.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747211856860.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747212372526.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747213677414.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747214300568.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747214329469.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747214616979.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747297341557.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747298481032.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747299036373.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747299735390.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747300140086.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747385664347.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747386959049.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747638635239.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747638823866.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747639107643.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747639830069.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747640212778.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747640804903.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747640838014.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747642737986.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747643839981.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747643871122.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747725041912.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747725892762.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747726362529.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747726423588.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747727710399.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747729359932.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747729386299.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747729458255.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747811457685.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747812259012.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747813848367.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747813206792.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747813236638.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747815312013.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747815845281.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747815908367.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747817587146.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747817659838.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747899851050.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747901048181.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747901106788.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747902009472.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748309464421.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748311358997.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748311399318.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748312803992.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748315214399.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748315853469.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748396612417.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748396630987.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748397117540.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748397615395.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748397712017.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748398349046.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748398451772.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748400504235.png">
<meta property="og:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748400612423.png">
<meta property="article:published_time" content="2025-05-08T06:44:44.000Z">
<meta property="article:modified_time" content="2025-05-28T03:03:36.618Z">
<meta property="article:author" content="rainstar">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://rain00star.github.io/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746688424952.png">
  
  
  
  <title>机器学习笔记 - RainStar&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"rain00star.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":100,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":false,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"noNmr9chT20dHBom1tUWnaWV-gzGzoHsz","app_key":"al8tnZ6DMkS0VQ1cKWpSbLvr","server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    
  



  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>RainStar</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>Links</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/frontback.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="机器学习笔记"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-05-08 14:44" pubdate>
          May 8, 2025 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          10k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          87 mins
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">机器学习笔记</h1>
            
            
              <div class="markdown-body">
                
                <p>[TOC]</p>
<h2 id="机器学习算法类型">机器学习算法类型</h2>
<ul>
<li>监督学习（常用）</li>
<li>非监督学习</li>
</ul>
<h3 id="监督学习">监督学习</h3>
<p>监督学习算法通过<strong>学习输入（x）到输出（y）的映射关系</strong>。在监督学习中，输入算法的为x，y（带标签）。</p>
<p>主要类型有回归和分类：</p>
<h4 id="回归regression">回归（regression）</h4>
<p>回归是监督学习的一种类型，任务是<strong>预测一个连续的数值</strong>。例如，根据房屋面积预测房价。在这个问题中，我们可以使用不同的算法来拟合数据（例如线性回归或曲线拟合）。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746688424952.png" srcset="/img/loading.gif" lazyload alt="1746688424952">
<figcaption aria-hidden="true">1746688424952</figcaption>
</figure>
<h4 id="分类classification">分类(classification)</h4>
<p>任务是<strong>将输入数据分配到有限数量的类别</strong>中。例如，根据电子邮件内容判断是否为垃圾邮件。</p>
<h3 id="无监督学习">无监督学习</h3>
<p>与监督学习不同，无监督学习算法<strong>不需要输出标签</strong>。它们的目标是<strong>从未标记的数据中找到某种结构、模式或者有趣的信息</strong>。</p>
<h4 id="聚类算法clustering">聚类算法（clustering）</h4>
<p>将未标记的数据分为不同的群组或簇。</p>
<h4 id="异常检测anomaly-detection">异常检测（anomaly detection）</h4>
<h4 id="降维dimensionality-reduction">降维(Dimensionality
reduction)</h4>
<h2 id="回归模型监督学习模型">回归模型(监督学习模型)</h2>
<h3 id="线性回归模型linear-regression-with-one-variable">线性回归模型（linear
regression with one variable）</h3>
<p>只有一个输入。</p>
<p>线性回归模型是通过<strong>拟合一条直线来描述数据之间的关系</strong>。</p>
<h4 id="tips专业术语">tips专业术语</h4>
<p><strong>训练集（Training
Set）</strong>是用于训练模型的数据集合。它通常是一个包含输入特征和对应输出值（或目标变量）的数据集，模型使用这些数据来学习如何将输入特征映射到正确的输出值。</p>
<p>我们通常用小写的<strong>x表示输入变量，也称为特征</strong>；用小写的<strong>y表示输出变量</strong>，也称为目标变量。<strong>数据集中的每一行代表一个训练样本</strong>。我们用小写的<strong>m表示训练样本的总数</strong>。为了表示<strong>特定的训练样本</strong>，我们使用带上标的表示法，如<strong>x<sup>(i)和y</sup>(i)</strong>，<strong>上标i表示第i个训练样本</strong>。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746696598520.png" srcset="/img/loading.gif" lazyload alt="1746696598520">
<figcaption aria-hidden="true">1746696598520</figcaption>
</figure>
<h4 id="模型表示">模型表示</h4>
<p>为了训练模型，您需要将训练集（包括输入特征和输出目标）提供给学习算法。然后，监督学习算法将生成某种函数（用f表示）。</p>
<p>函数f被称为模型（model）。X被称为输入或输入特征（feature），模型的输出是预测值y-hat。</p>
<p>我们如何表示函数f？</p>
<p>我们使用一条直线作为f。您的函数可以写成f(x) = w*x + b。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746697450500.png" srcset="/img/loading.gif" lazyload alt="1746697450500">
<figcaption aria-hidden="true">1746697450500</figcaption>
</figure>
<h4 id="实验-numpymatplotlib">实验 numpy,matplotlib</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.style.use(<span class="hljs-string">&#x27;./deeplearning.mplstyle&#x27;</span>)<br><br><span class="hljs-comment">#训练集</span><br>x_train = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>])<br>y_train = np.array([<span class="hljs-number">300.0</span>, <span class="hljs-number">500.0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x_train = <span class="hljs-subst">&#123;x_train&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;y_train = <span class="hljs-subst">&#123;y_train&#125;</span>&quot;</span>)<br><span class="hljs-comment">###x_train = [1. 2.]</span><br><span class="hljs-comment">###y_train = [300. 500.]</span><br><br><br><span class="hljs-comment">#训练样本总数</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;x_train.shape: <span class="hljs-subst">&#123;x_train.shape&#125;</span>&quot;</span>)       <span class="hljs-comment">#返回一个元组</span><br>m = x_train.shape[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Number of training examples is: <span class="hljs-subst">&#123;m&#125;</span>&quot;</span>)<br><span class="hljs-comment">###x_train.shape: (2,)</span><br><span class="hljs-comment">###Number of training examples is: 2</span><br><span class="hljs-comment">#或者直接使用len</span><br>m = <span class="hljs-built_in">len</span>(x_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Number of training examples is: <span class="hljs-subst">&#123;m&#125;</span>&quot;</span>)<br><br><span class="hljs-comment">#创建m维空数组</span><br>m = x.shape[<span class="hljs-number">0</span>]<br>f_wb = np.zeros(m)<br><br><span class="hljs-comment">#画图</span><br>plt.plot(x_train, tmp_f_wb, c=<span class="hljs-string">&#x27;b&#x27;</span>,label=<span class="hljs-string">&#x27;Our Prediction&#x27;</span>)<span class="hljs-comment">#画直线,蓝色,标签名为Our Prediction</span><br>plt.scatter(x_train, y_train, marker=<span class="hljs-string">&#x27;x&#x27;</span>, c=<span class="hljs-string">&#x27;r&#x27;</span>,label=<span class="hljs-string">&#x27;Actual Values&#x27;</span>) <span class="hljs-comment">#画点,红叉点,标签名为Actual Values</span><br><br>plt.title(<span class="hljs-string">&quot;Housing Prices&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Price (in 1000s of dollars)&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Size (1000 sqft)&#x27;</span>)<br>plt.legend()         <span class="hljs-comment">#显示图标                                                </span><br>plt.show()<br></code></pre></td></tr></table></figure>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746699765119.png" srcset="/img/loading.gif" lazyload alt="1746699765119">
<figcaption aria-hidden="true">1746699765119</figcaption>
</figure>
<h4 id="成本函数-平均误差成本函数squared-error-cost-function">成本函数-平均误差成本函数（squared
error cost function）</h4>
<p>平方误差成本函数（Squared error cost
function）是一种用于<strong>衡量模型预测结果与真实结果之间误差大小</strong>的函数。在一元线性回归模型中，通常使用平方误差成本函数作为优化目标，目的是<strong>最小化预测值与真实值之间的平方误差</strong>。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746772964358.png" srcset="/img/loading.gif" lazyload alt="1746772964358">
<figcaption aria-hidden="true">1746772964358</figcaption>
</figure>
<p>模型与成本参数之间的直观关系：（固定b为0时）</p>
<p>模型f对于每个固定的w，是关于x的函数；</p>
<p>成本函数J对于固定的训练集，是关于w的函数；</p>
<p>一个w对应f的一条直线，对应J的一个点。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746773968352.png" srcset="/img/loading.gif" lazyload alt="1746773968352">
<figcaption aria-hidden="true">1746773968352</figcaption>
</figure>
<p>可视化成本函数：（不固定b为0）</p>
<p>二维等高线图的每一个椭圆表示J值相等，最中心点为J值最小点。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746774767762.png" srcset="/img/loading.gif" lazyload alt="1746774767762">
<figcaption aria-hidden="true">1746774767762</figcaption>
</figure>
<h2 id="梯度下降gradient-descent">梯度下降(gradient descent)</h2>
<p>梯度下降算法是一种<strong>用于寻找使损失函数最小化的参数 w 和 b
的值</strong>的方法。它在机器学习中得到了广泛应用，不仅用于线性回归，还用于训练最先进的神经网络模型（梯度下降算法<strong>可以应用于任何函数的最小化问题</strong>，包括具有多个参数的模型的损失函数。）。</p>
<p>梯度下降的过程：</p>
<p>在梯度下降算法中，首先<strong>选择一组初始参数 w 和
b</strong>。然后，通过每次微调参数 w 和 b 以减小损失函数 J(w, b)
的值，直到 J
达到或接近最小值。<strong>在每一步，选择使损失函数下降最快的方向进行更新。</strong></p>
<p><strong>梯度下降算法可能会根据初始参数值的选择而收敛到不同的局部最小值。</strong></p>
<h3 id="梯度下降实现">梯度下降实现</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746781317379.png" srcset="/img/loading.gif" lazyload alt="1746781317379">
<figcaption aria-hidden="true">1746781317379</figcaption>
</figure>
<h4 id="学习率α">学习率α：</h4>
<p><strong>学习率（learning
rate）控制每次迭代时参数更新的步长。</strong>较大的学习率表示更大的步长，较小的学习率表示更小的步长。选择合适的学习率很重要，因为它影响了梯度下降算法的收敛速度和稳定性。
如果<strong>学习率过小</strong>，那么模型的<strong>收敛速度会变得非常缓慢</strong>，需要更多的迭代次数才能收敛到最优解，同时可能会<strong>陷入局部最优解</strong>。如果学习率过大，那么模型的<strong>收敛速度会变得非常快</strong>，但可能会导致模型<strong>在最优解附近震荡甚至无法收敛</strong>。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746783342411.png" srcset="/img/loading.gif" lazyload alt="1746783342411">
<figcaption aria-hidden="true">1746783342411</figcaption>
</figure>
<p>在逼近局部最小值时，梯度下降算法会自动采取较小的步长。即使学习率 α
保持固定，导数值会随着接近最小值而变小，从而使更新步长变小。<strong>当参数已经位于局部最小值时，步长会变为0。</strong></p>
<h4 id="更新方式同步更新">更新方式：同步更新</h4>
<p>同步更新(Simultaneously update)： 在实现梯度下降时，应同时更新参数 w
和
b。这意味着在计算新值之前，不应更改任何一个参数。同步更新有助于正确实现梯度下降算法.</p>
<h4 id="偏导数">偏导数</h4>
<p>指导正确的更新方向。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1746782075789.png" srcset="/img/loading.gif" lazyload alt="1746782075789">
<figcaption aria-hidden="true">1746782075789</figcaption>
</figure>
<h4 id="具体实现针对线性回归模型平均误差成本函数1747034769708">具体实现—针对线性回归模型，平均误差成本函数<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747034769708.png" srcset="/img/loading.gif" lazyload alt="1747034769708"></h4>
<p>平方误差成本函数在线性回归中具有一个特殊性质：它是一个<strong>凸函数</strong>。这意味着成本函数是一个碗状函数，<strong>仅有一个全局最小值</strong>，没有其他局部最小值。</p>
<p>当在凸函数上实现梯度下降时，<strong>只要学习率选择得当</strong>，算法总是会收敛到全局最小值。</p>
<h4 id="批量梯度下降batch-gradient-descent">批量梯度下降(batch gradient
descent)</h4>
<p>批量梯度下降是指在每次梯度下降更新中，我们查看所有训练样本，而不仅仅是训练数据的一个子集。</p>
<h2 id="多输入变量回归问题">多输入变量回归问题</h2>
<h3 id="符号表示">符号表示</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747037413940.png" srcset="/img/loading.gif" lazyload alt="1747037413940">
<figcaption aria-hidden="true">1747037413940</figcaption>
</figure>
<h3 id="模型表示-1">模型表示</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747037483049.png" srcset="/img/loading.gif" lazyload alt="1747037483049">
<figcaption aria-hidden="true">1747037483049</figcaption>
</figure>
<p>f(X) = <strong>W</strong> • <strong>X</strong> +
b。这里的点乘（•）表示两个向量的对应元素相乘后求和。</p>
<h3 id="向量化">向量化</h3>
<p>可以使用<code>numpy</code>库来实现快速计算：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747038198520.png" srcset="/img/loading.gif" lazyload alt="1747038198520">
<figcaption aria-hidden="true">1747038198520</figcaption>
</figure>
<p>背后原理：硬件实现并行计算</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747038691415.png" srcset="/img/loading.gif" lazyload alt="1747038691415">
<figcaption aria-hidden="true">1747038691415</figcaption>
</figure>
<h3 id="梯度下降">梯度下降</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747041118033.png" srcset="/img/loading.gif" lazyload alt="1747041118033">
<figcaption aria-hidden="true">1747041118033</figcaption>
</figure>
<h4 id="特征缩放加速收敛">特征缩放—加速收敛</h4>
<ul>
<li>特征值与相应的权重关系：</li>
</ul>
<p>当特征值的范围较大时，我们需要选择相对较小的参数值；而特征值范围较小时，参数值可能会相对较大。（这也是梯度下降算法的自动选择）</p>
<ul>
<li>存在问题：</li>
</ul>
<p><strong>当特征值范围相差很大时，损失函数的等高线可能呈椭圆形，导致梯度下降算法在寻找全局最小值时需要较长时间。</strong></p>
<p>（w1任何一点微小变化都会对成本函数的值造成较大影响，因为w1所乘的特征值很大，反之同理）</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747041904302.png" srcset="/img/loading.gif" lazyload alt="1747041904302">
<figcaption aria-hidden="true">1747041904302</figcaption>
</figure>
<ul>
<li>解决问题：特征缩放</li>
</ul>
<p>可以对特征进行缩放，使得它们的取值范围相对相似。这样做可以使损<strong>失函数的等高线更接近圆形，从而加速梯度下降算法</strong>。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747042064277.png" srcset="/img/loading.gif" lazyload alt="1747042064277">
<figcaption aria-hidden="true">1747042064277</figcaption>
</figure>
<ul>
<li>特征缩放的具体方法</li>
</ul>
<p>除以最大值：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747042683490.png" srcset="/img/loading.gif" lazyload alt="1747042683490">
<figcaption aria-hidden="true">1747042683490</figcaption>
</figure>
<p>均值归一化：减去每个特征的均值，然后除以最大值和最小值之间的差值。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747042730321.png" srcset="/img/loading.gif" lazyload alt="1747042730321">
<figcaption aria-hidden="true">1747042730321</figcaption>
</figure>
<p>Z得分归一化（Z-score
Normalization）：对于每个特征值，减去其均值并除以其标准差。</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs tex"><span class="hljs-comment">%标准差</span><br>标准差（standard deviation）是衡量一组数据离散程度的常用指标，是方差的算术平方根。**标准差越大，说明数据的离散程度越大，数据点更分散；标准差越小，说明数据的离散程度越小，数据点更集中。标准差的计算公式为所有数据与均值之差的平方和除以样本大小的平方根。<br></code></pre></td></tr></table></figure>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747042782174.png" srcset="/img/loading.gif" lazyload alt="1747042782174">
<figcaption aria-hidden="true">1747042782174</figcaption>
</figure>
<ul>
<li>特征缩放的目标：通常情况下，我们希望特征值范围在大约 -1 到 1
之间。这些值可以有一定的灵活性，例如 -3 到 3 或者 -0.3 到 0.3
也是可以接受的。</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747042823630.png" srcset="/img/loading.gif" lazyload alt="1747042823630">
<figcaption aria-hidden="true">1747042823630</figcaption>
</figure>
<h4 id="检查梯度下降是否收敛">检查梯度下降是否收敛</h4>
<ul>
<li>学习曲线</li>
</ul>
<p>绘制学习曲线：在每次梯度下降迭代后，绘制成本函数 J
的值。<strong>水平轴表示梯度下降的迭代次数，垂直轴表示成本函数 J
的值</strong>；</p>
<p>观察学习曲线：当梯度下降运行正确时，<strong>成本函数 J
应该在每次迭代后都减少</strong>。如果 J 在某次迭代后增加，这意味着学习率
Alpha 可能选择不当（通常过大），或者代码中可能存在错误；</p>
<p>判断梯度下降收敛：观察学习曲线，当<strong>曲线趋于平缓且不再降低时，可以认为梯度下降已经收敛。</strong></p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747121658737.png" srcset="/img/loading.gif" lazyload alt="1747121658737">
<figcaption aria-hidden="true">1747121658737</figcaption>
</figure>
<ul>
<li>自动收敛测试</li>
</ul>
<p><strong>设定一个小的阈值（如 0.001 或 10^-3），如果成本函数 J
在一次迭代中减少的量小于这个阈值，可以认为已经收敛。</strong>但是，选择合适的阈值可能比较困难，因此通常更倾向于观察学习曲线，而不是依赖自动收敛测试。</p>
<h3 id="正规方程法normal-equation">正规方程法(Normal equation)</h3>
<p>正规方程法是求解线性回归中 w 和 b
的另一种方法。与梯度下降法不同，正规方程法不需要迭代，而是直接通过一次计算来求解
w 和
b。正规方程法的局限性在于，它<strong>只适用于线性回归问题</strong>，而且<strong>在特征数量较大时计算速度较慢</strong>。</p>
<h3 id="学习率的选择">学习率的选择</h3>
<p>当成本函数 J
在梯度下降的迭代过程中时而增加时而减少（或者一直增加），这意味着梯度下降没有正常工作。这可能是代码中的错误，或者学习率太大。</p>
<p>选择合适的学习率：（选择偏小的，再选择一个偏大的，两者之间权衡，取得最合适的值）</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747122250668.png" srcset="/img/loading.gif" lazyload alt="1747122250668">
<figcaption aria-hidden="true">1747122250668</figcaption>
</figure>
<h3 id="特征工程">特征工程</h3>
<p>特征工程是使<strong>用你对问题的知识或直觉来设计新特征的过程，</strong>通常通过转换或组合原始特征使学习算法能更容易地做出准确预测。</p>
<p>例子：</p>
<p>预测房价。假设你有两个特征：x_1（房子所在土地的宽度或前沿宽度）和
x_2（房子所在土地的深度）。你可以构建一个模型：f(x) = w_1x_1 + w_2x_2 +
b。但是，你也可以通过组合这些特征来创建一个更有效的模型。如下：</p>
<p>你可以计算土地的面积（x_1 * x_2），并将其定义为新特征
x_3。然后，你可以构建一个包含这个新特征的模型：f(x) = w_1x_1 + w_2x_2 +
w_3x_3 + b。这样，模型可以根据数据选择参数 w_1、w_2 和
w_3，以判断土地的宽度、深度或面积对预测房价的重要性。</p>
<h3 id="多项式回归">多项式回归</h3>
<p>多项式回归允许你拟合非线性函数，而不仅仅是直线。例如下列的函数：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747123119951.png" srcset="/img/loading.gif" lazyload alt="1747123119951">
<figcaption aria-hidden="true">1747123119951</figcaption>
</figure>
<p>当使用原始特征的<strong>平方和立方和</strong>等幂作为特征时，<strong>特征缩放</strong>变得非常重要。</p>
<h2 id="分类">分类</h2>
<p>在分类问题中，我们关注<strong>二元分类问题，其中只有两个可能的输出类别</strong>。这些类别通常用0和1表示，0表示负类（或否/假），1表示正类（或是/真）。例如，在垃圾邮件分类问题中，非垃圾邮件（否/假）是负类，垃圾邮件（是/真）是正类。</p>
<p>尝试使用线性回归解决分类问题可能会导致问题：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747126935553.png" srcset="/img/loading.gif" lazyload alt="1747126935553">
<figcaption aria-hidden="true">1747126935553</figcaption>
</figure>
<p>当我们添加一个新的训练样本时，线性回归的最佳拟合线可能会改变，导致分类边界的改变。这表明线性回归不适合处理分类问题。</p>
<h3 id="逻辑回归模型-sigmoid函数">逻辑回归模型-sigmoid函数</h3>
<p>sigmoid函数：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747127662190.png" srcset="/img/loading.gif" lazyload alt="1747127662190">
<figcaption aria-hidden="true">1747127662190</figcaption>
</figure>
<p>逻辑回归函数：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747127561170.png" srcset="/img/loading.gif" lazyload alt="1747127561170">
<figcaption aria-hidden="true">1747127561170</figcaption>
</figure>
<p>逻辑回归的算法可以分为两步：</p>
<ul>
<li><strong>计算 z = wx + b，其中 w 和 x 是特征权重和输入特征，b
是偏置项。</strong></li>
<li><strong>将 z 代入 Sigmoid 函数，得到 g(z)
的值，这个值就是逻辑回归模型的输出（y=1的概率值）。</strong></li>
</ul>
<h3 id="决策边界">决策边界</h3>
<p>我们要找到一个阈值，使得当 <strong>g(z) ≥ 0.5 时，我们将其分类为类别
1，否则分类为类别 0。</strong>现在，让我们推导一下：</p>
<ol type="1">
<li>g(z) ≥ 0.5</li>
<li>由于我们使用 sigmoid 函数，它在 z=0 时等于 0.5，所以当 z≥0
时，g(z)≥0.5。</li>
<li>现在，我们用 f(x) 替换 z，即 f(x) = <strong>w</strong>*
<strong>x</strong> + b。</li>
<li>当 f(x)≥0 时，g(f(x))≥0.5。</li>
</ol>
<p>这里的推导说明，<strong>我们可以通过检查 f(x) 是否大于等于 0
来确定数据点属于哪个类别。</strong>如果 f(x)≥0，则数据点属于类别
1，否则属于类别 0。<strong>这个条件定义了我们的决策边界</strong>。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747128421745.png" srcset="/img/loading.gif" lazyload alt="1747128421745">
<figcaption aria-hidden="true">1747128421745</figcaption>
</figure>
<p>边界：z=0即<code>wx+b=0</code>所表示的线</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747128554959.png" srcset="/img/loading.gif" lazyload alt="1747128554959">
<figcaption aria-hidden="true">1747128554959</figcaption>
</figure>
<p>非线性决策边界：意味着这个<strong>边界不是一条直线</strong>（对于二维数据）或者平面（对于三维数据）等线性结构，而是一个更复杂的形状。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747128606609.png" srcset="/img/loading.gif" lazyload alt="1747128606609">
<figcaption aria-hidden="true">1747128606609</figcaption>
</figure>
<h3 id="成本函数">成本函数</h3>
<p>平均误差成本函数不适合分类问题的成本函数，因为该函数不是凸函数，存在多个局部最小值，如下图：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747207959425.png" srcset="/img/loading.gif" lazyload alt="1747207959425">
<figcaption aria-hidden="true">1747207959425</figcaption>
</figure>
<ul>
<li>单个训练样本的损失函数：</li>
</ul>
<p>根据数据集中的标签定义不同的损失函数</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747208024315.png" srcset="/img/loading.gif" lazyload alt="1747208024315">
<figcaption aria-hidden="true">1747208024315</figcaption>
</figure>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747208048886.png" srcset="/img/loading.gif" lazyload alt="1747208048886">
<figcaption aria-hidden="true">1747208048886</figcaption>
</figure>
<ul>
<li>总成本函数：</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747208097956.png" srcset="/img/loading.gif" lazyload alt="1747208097956">
<figcaption aria-hidden="true">1747208097956</figcaption>
</figure>
<p>简化写法：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747208465141.png" srcset="/img/loading.gif" lazyload alt="1747208465141">
<figcaption aria-hidden="true">1747208465141</figcaption>
</figure>
<h3 id="梯度下降-1">梯度下降</h3>
<p>更新公式如下：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747210389610.png" srcset="/img/loading.gif" lazyload alt="1747210389610">
<figcaption aria-hidden="true">1747210389610</figcaption>
</figure>
<h2 id="过拟合问题overfitting">过拟合问题（overfitting）</h2>
<ul>
<li><strong>欠拟合（Underfitting）</strong>：当一个学习算法不能很好地捕捉到训练数据中的模式，导致模型在训练数据上的表现也不好时，我们称其为欠拟合。<strong>欠拟合的模型在训练数据和测试数据上的误差都可能很大。欠</strong>拟合的另一个术语是<strong>高偏差（High
Bias）</strong>。</li>
<li><strong>过拟合（Overfitting）</strong>：当一个学习算法在训练数据上表现得过于优秀，以至于它捕捉到了训练数据中的噪声，而不能很好地泛化到新的数据时，我们称其为过拟合。<strong>过拟合的模型在训练数据上的误差可能非常小，但在测试数据上的误差可能很大</strong>。过拟合的另一个术语是<strong>高方差（High
Variance）</strong>。</li>
</ul>
<p>回归问题：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747211824874.png" srcset="/img/loading.gif" lazyload alt="1747211824874">
<figcaption aria-hidden="true">1747211824874</figcaption>
</figure>
<p>分类问题：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747211856860.png" srcset="/img/loading.gif" lazyload alt="1747211856860">
<figcaption aria-hidden="true">1747211856860</figcaption>
</figure>
<h3 id="问题解决">问题解决</h3>
<p>三种方法：</p>
<ul>
<li><strong>收集更多数据</strong>：增加训练数据可以帮助模型学习更普遍的模式，从而降低过拟合的风险。</li>
<li><strong>减少特征数量</strong>（特征选择）：通过选择和使用特征子集，可以<strong>降低模型复杂度</strong>并减少过拟合。这可以通过直观地选择最相关的特征或使用算法自动选择特征来实现。</li>
<li><strong>正则化（Regularization）</strong>：正则化是一种减小模型参数值的方法，从而防止特征对模型产生过大的影响，这有时会导致过拟合。通过限制模型参数的大小，可以<strong>在保留所有特征的同时减小过拟合</strong>的风险。</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747212372526.png" srcset="/img/loading.gif" lazyload alt="1747212372526">
<figcaption aria-hidden="true">1747212372526</figcaption>
</figure>
<h4 id="正则化代价函数">正则化代价函数</h4>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747213677414.png" srcset="/img/loading.gif" lazyload alt="1747213677414">
<figcaption aria-hidden="true">1747213677414</figcaption>
</figure>
<p><strong>lambda</strong>(λ) 是正则化参数：</p>
<p>它是一个非负数，用于控制正则化项的权重。<strong>λ
的值越大</strong>，正则化项对损失函数的影响越大，从而更强烈地约束模型参数的大小，降低模型复杂度。<strong>（欠拟合）</strong>相反，当
<strong>λ
的值越小</strong>，正则化项对损失函数的影响越小，模型参数将趋向于更复杂的解<strong>（过拟合）</strong>。通过调整
λ
的值，我们可以在模型的偏差（bias）和方差（variance）之间找到一个平衡点，从而获得性能更好的模型。</p>
<h4 id="正则化线性回归">正则化线性回归</h4>
<p>计算公式：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747214300568.png" srcset="/img/loading.gif" lazyload alt="1747214300568">
<figcaption aria-hidden="true">1747214300568</figcaption>
</figure>
<p>背后的原理：每次迭代w乘以一个接近1但小于1的数进行缩小</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747214329469.png" srcset="/img/loading.gif" lazyload alt="1747214329469">
<figcaption aria-hidden="true">1747214329469</figcaption>
</figure>
<h4 id="正则化逻辑回归">正则化逻辑回归</h4>
<p>计算公式同线性回归。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747214616979.png" srcset="/img/loading.gif" lazyload alt="1747214616979">
<figcaption aria-hidden="true">1747214616979</figcaption>
</figure>
<h2 id="神经网络">神经网络</h2>
<h3 id="结构概述">结构概述</h3>
<p>通常由多个层组成，包括输入层、隐藏层和输出层。</p>
<ul>
<li><strong>输入层</strong>：神经网络的第一层，负责接收原始数据（如图像、文本或音频）并将其传递给下一层。</li>
<li><strong>输出层</strong>：神经网络的最后一层，负责根据前面层的处理结果生成预测或分类输出。</li>
<li><strong>隐藏层</strong>：在输入层和输出层之间的层。隐藏层可以有一个或多个，每个隐藏层包含多个神经元。隐藏层负责对输入数据进行处理和转换，<strong>以提取有意义的特征。（自动学习特征）</strong></li>
<li><strong>神经元</strong>：神经网络中的基本计算单元，负责接收输入、进行计算并产生输出（激活值）。<strong>神经元之间通过权重连接，以传递和调整信号</strong>。</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747297341557.png" srcset="/img/loading.gif" lazyload alt="1747297341557">
<figcaption aria-hidden="true">1747297341557</figcaption>
</figure>
<h3 id="应用图像识别">应用：图像识别</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747298481032.png" srcset="/img/loading.gif" lazyload alt="1747298481032">
<figcaption aria-hidden="true">1747298481032</figcaption>
</figure>
<p><strong>隐藏层学习特征：神经网络的隐藏层可以自动学习特征。</strong>在第一层隐藏层，神经元可能学习检测图像中的短边缘和线条；在第二层隐藏层，神经元可能学习检测面部的各个部分（如眼睛、鼻子等）；在第三层隐藏层，神经元可能学习检测更完整的面部形状。</p>
<h3 id="神经网络模型">神经网络模型</h3>
<ul>
<li>基本模型（layer1展开）：两层模型。不包括输入层</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747299036373.png" srcset="/img/loading.gif" lazyload alt="1747299036373">
<figcaption aria-hidden="true">1747299036373</figcaption>
</figure>
<p>符号约定：<strong>使用上标方括号表示与特定层相关的数量</strong>。例如，a<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> 表示与第一层相关的激活值，w<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> 和 b<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>
表示与第一层相关的权重和偏置值。</p>
<ul>
<li>复杂模型：四层模型</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747299735390.png" srcset="/img/loading.gif" lazyload alt="1747299735390">
<figcaption aria-hidden="true">1747299735390</figcaption>
</figure>
<p>激活函数：激活函数（如 Sigmoid
函数g）用于计算神经元的输出。激活函数的作用是将神经元的输入经过非线性变换后输出。激活函数不仅限于
Sigmoid 函数，还有其他激活函数，如 ReLU、tanh 等。</p>
<h3 id="前向传播">前向传播</h3>
<p>前向传播算法是一种计算神经网络输出的方法。<strong>从输入层开始，逐层计算每一层的激活值，直到输出层。</strong>算法从左到右进行计算，因此称为前向传播。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747300140086.png" srcset="/img/loading.gif" lazyload alt="1747300140086">
<figcaption aria-hidden="true">1747300140086</figcaption>
</figure>
<p>在这个例子中，输入是一个8x8的图像，神经网络包含两个隐藏层（第一个隐藏层有25个神经元，第二个隐藏层有15个神经元）以及一个输出层。<strong>前向传播算法首先计算第一隐藏层的激活值
a1，然后计算第二隐藏层的激活值 a2，最后计算输出层的激活值
a3</strong>。</p>
<h4 id="利用tensorflow实现前向传播">利用TensorFlow实现前向传播</h4>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747385664347.png" srcset="/img/loading.gif" lazyload alt="1747385664347">
<figcaption aria-hidden="true">1747385664347</figcaption>
</figure>
<h4 id="利用python实现前向传播">利用python实现前向传播</h4>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747386959049.png" srcset="/img/loading.gif" lazyload alt="1747386959049">
<figcaption aria-hidden="true">1747386959049</figcaption>
</figure>
<h3 id="agi">AGI</h3>
<p><strong>人工窄智能（ANI）和人工通用智能（AGI）</strong>。</p>
<p>ANI 是指在特定任务上表现出色的 AI
系统，如智能音箱、自动驾驶汽车、网络搜索等。近年来，ANI
取得了巨大进展，并为世界带来了很多价值。</p>
<p>而 AGI 是指能够完成任何典型人类能做的事情的 AI 系统。尽管 ANI
取得了很多进展，但我们在 AGI 领域的进展仍不明朗</p>
<h3 id="矩阵乘法">矩阵乘法</h3>
<h4 id="向量点积">向量点积</h4>
<p>向量点积是<strong>将两个具有相同数量元素的向量进行对应元素相乘并求和的操作</strong>。向量点积还可以表示为一个向量的转置与另一个向量相乘。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747638635239.png" srcset="/img/loading.gif" lazyload alt="1747638635239">
<figcaption aria-hidden="true">1747638635239</figcaption>
</figure>
<h4 id="矩阵相乘">矩阵相乘</h4>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747638823866.png" srcset="/img/loading.gif" lazyload alt="1747638823866">
<figcaption aria-hidden="true">1747638823866</figcaption>
</figure>
<h2 id="训练神经网络">训练神经网络</h2>
<h3 id="训练代码">训练代码</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747639107643.png" srcset="/img/loading.gif" lazyload alt="1747639107643">
<figcaption aria-hidden="true">1747639107643</figcaption>
</figure>
<ul>
<li>步骤1：指定模型。这与上周看到的内容相似，我们要求<strong>TensorFlow将神经网络的三层顺序连接在一起。</strong></li>
<li>步骤2：编译模型。<strong>编译模型的关键在于指定要使用的损失函数。</strong>在这个例子中，我们使用了二元交叉熵损失函数（即逻辑回归的成本函数）。</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747639830069.png" srcset="/img/loading.gif" lazyload alt="1747639830069">
<figcaption aria-hidden="true">1747639830069</figcaption>
</figure>
<ul>
<li>步骤3：拟合模型。调用fit函数，告诉TensorFlow使用指定的数据集X和Y以及步骤2中指定的损失函数来拟合模型。此外，<strong>还需要指定要运行的迭代次数（称为epochs）</strong>。</li>
</ul>
<h3 id="激活函数">激活函数</h3>
<h4 id="常用">常用：</h4>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747640212778.png" srcset="/img/loading.gif" lazyload alt="1747640212778">
<figcaption aria-hidden="true">1747640212778</figcaption>
</figure>
<h4 id="如何选择分隐藏层和输出层">如何选择：分隐藏层和输出层</h4>
<ul>
<li>输出层：根据标签</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747640804903.png" srcset="/img/loading.gif" lazyload alt="1747640804903">
<figcaption aria-hidden="true">1747640804903</figcaption>
</figure>
<ul>
<li>隐藏层：常用ReLU</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747640838014.png" srcset="/img/loading.gif" lazyload alt="1747640838014">
<figcaption aria-hidden="true">1747640838014</figcaption>
</figure>
<h4 id="为什么需要激活函数">为什么需要激活函数：</h4>
<p>如果在<strong>神经网络中的每个神经元都使用线性激活函数，那么神经网络将变得与线性回归模型没有任何区别，</strong>因此无法捕捉到更复杂的模式。这意味着使用神经网络将失去意义，因为它无法拟合比线性回归更复杂的模型。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747642737986.png" srcset="/img/loading.gif" lazyload alt="1747642737986">
<figcaption aria-hidden="true">1747642737986</figcaption>
</figure>
<p>在一个简单的神经网络示例中，<strong>当我们在所有神经元中使用线性激活函数时，输出
a2 只是输入 x
的线性函数。</strong>这说明我们可以用线性回归模型代替包含一个隐藏层和一个输出层的神经网络。这个结论源于线性代数的一个事实：<strong>线性函数的线性函数本身是一个线性函数</strong>。因此，具有多个层的神经网络不能让神经网络计算更复杂的特征，也不能学习比线性函数更复杂的模式。</p>
<p>如果在具有多个层的神经网络中使用线性激活函数，那么该模型将完全等同于线性回归。<strong>如果我们仅在隐藏层使用线性激活函数，而在输出层使用
logistic 激活函数，那么模型将等同于 logistic
回归。</strong>因此，<strong>建议在神经网络的隐藏层中不要使用线性激活函数，而使用
ReLU 等非线性激活函数。</strong></p>
<h2 id="softmax回归算法逻辑回归模型拓展多元分类">softmax回归算法（逻辑回归模型拓展）——多元分类</h2>
<h3 id="激活函数-1">激活函数：</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747643839981.png" srcset="/img/loading.gif" lazyload alt="1747643839981">
<figcaption aria-hidden="true">1747643839981</figcaption>
</figure>
<h3 id="成本函数损失函数平均">成本函数：（损失函数平均）</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747643871122.png" srcset="/img/loading.gif" lazyload alt="1747643871122">
<figcaption aria-hidden="true">1747643871122</figcaption>
</figure>
<h3 id="改进tensorflow代码">改进Tensorflow代码：</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747725041912.png" srcset="/img/loading.gif" lazyload alt="1747725041912">
<figcaption aria-hidden="true">1747725041912</figcaption>
</figure>
<p>将激活函数和损失函数组合在一起，可以改进原始的softmax实现。这种方法避免了计算中间值a，从而提高了数值稳定性。<strong>要实现这一点，需要将输出层的激活函数更改为线性激活函数，并将损失函数设置为from_logits=True</strong>。虽然这使代码更难阅读，但实际上，这样的实现更精确且数值稳定。</p>
<p>优化实现的<strong>输出层不再输出概率A_1至A_10，而是输出z_1至z_10</strong>。<strong>为了获取概率值，需要对输出层的值进行额外处理</strong>。</p>
<h3 id="多标签分类问题">多标签分类问题</h3>
<p><strong>与多类分类问题不同，多标签分类问题允许为每个输入分配多个标签。</strong>例如，在自动驾驶汽车的场景中，给定一张照片，您可能希望判断图片中是否存在汽车、公共汽车和行人。在这种情况下，每个输入图像都可能同时具有多个标签。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747725892762.png" srcset="/img/loading.gif" lazyload alt="1747725892762">
<figcaption aria-hidden="true">1747725892762</figcaption>
</figure>
<p>注意：最后使用的sigmoid函数，不是softmax函数。</p>
<h2 id="高级优化方法-adam">高级优化方法-Adam</h2>
<p>在某些情况下，<strong>梯度下降可能需要更大或更小的学习率α来更快地达到最优解。</strong>如果梯度下降沿着大致相同的方向连续移动，我们可能希望增加学习率。相反，如果参数在两个方向之间来回震荡，我们可能希望减小学习率。</p>
<p><strong>Adam算法（Adaptive Moment
Estimation）是一种自适应优化算法，可以根据参数的变化自动调整学习率。</strong>Adam算法为模型的每个参数使用不同的学习率。如果参数似乎始终沿着大致相同的方向移动，那么我们可以加快该方向上的学习率。相反，如果参数在两个方向之间来回震荡，我们可以减小相应参数的学习率。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747726362529.png" srcset="/img/loading.gif" lazyload alt="1747726362529">
<figcaption aria-hidden="true">1747726362529</figcaption>
</figure>
<h3 id="算法实现">算法实现</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747726423588.png" srcset="/img/loading.gif" lazyload alt="1747726423588">
<figcaption aria-hidden="true">1747726423588</figcaption>
</figure>
<h2 id="附加层卷积神经网络">附加层—卷积神经网络</h2>
<p><strong>卷积层是神经网络中的一种类型的层，它的特点是每个神经元只看一小部分输入</strong>。</p>
<p>使用卷积层可以<strong>加速计算速度</strong>，因为每个神经元只看一小部分输入，而不是全部输入。此外，卷积层可以减少训练数据的需求或减少过拟合的发生。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747727710399.png" srcset="/img/loading.gif" lazyload alt="1747727710399">
<figcaption aria-hidden="true">1747727710399</figcaption>
</figure>
<h2 id="机器学习算法诊断改进">机器学习算法诊断、改进</h2>
<h3 id="模型评估">模型评估</h3>
<p>将<strong>训练数据集分为两个子集：训练集和测试集。在训练集上训练模型，然后在测试集上进行评估，计算出测试误差。测试误差通常是通过计算测试集上的平均误差来计算的</strong>。</p>
<h4 id="线性回归模型">线性回归模型</h4>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747729359932.png" srcset="/img/loading.gif" lazyload alt="1747729359932">
<figcaption aria-hidden="true">1747729359932</figcaption>
</figure>
<h4 id="逻辑回归模型">逻辑回归模型</h4>
<p>使用成本函数：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747729386299.png" srcset="/img/loading.gif" lazyload alt="1747729386299">
<figcaption aria-hidden="true">1747729386299</figcaption>
</figure>
<p>直接计算错误分类的比例：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747729458255.png" srcset="/img/loading.gif" lazyload alt="1747729458255">
<figcaption aria-hidden="true">1747729458255</figcaption>
</figure>
<h3 id="模型选择和训练交叉验证测试集">模型选择和训练交叉验证测试集</h3>
<p>将数据集分为三个子集，<strong>训练集（training
set）、交叉验证集（cross-validation set）和测试集（test
set）</strong>。</p>
<p>训练误差（training error）：在训练集上计算模型的误差。</p>
<p>交叉验证误差（cross-validation
error）：在交叉验证集上计算模型的误差。也被称为验证误差（validation
error）或开发集误差（development set error）。</p>
<p>测试误差（test
error）：在测试集上计算模型的误差。<strong>确保客观公正。</strong></p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747811457685.png" srcset="/img/loading.gif" lazyload alt="1747811457685">
<figcaption aria-hidden="true">1747811457685</figcaption>
</figure>
<p><strong>前两个数据集用于模型训练和选择，最后一个数据集用于模型评估。</strong></p>
<h3 id="评估指标">评估指标</h3>
<h4 id="偏差和方差">偏差和方差</h4>
<p>通过<code>Jtrain</code>和<code>Jcv</code>的大小进行判断，具体如下：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747812259012.png" srcset="/img/loading.gif" lazyload alt="1747812259012">
<figcaption aria-hidden="true">1747812259012</figcaption>
</figure>
<p>判断基准：</p>
<p>我们使用了训练错误（training error）和交叉验证错误（cross-validation
error）来评估算法的性能。但是，仅依赖这两个错误率可能无法准确判断算法是否存在高偏差或高方差问题，因此我们<strong>需要建立一个基线性能（baseline
level of
performance）作为参考。通常，人类水平的性能可以作为基线性能或者选择其他竞争学习算法作为基准。</strong></p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747813848367.png" srcset="/img/loading.gif" lazyload alt="1747813848367">
<figcaption aria-hidden="true">1747813848367</figcaption>
</figure>
<p>为了判断算法是否存在高偏差或高方差问题，我们需要关注以下两个关键量：</p>
<ul>
<li>训练错误与基线性能之间的差距：如果这个差距很大，说明算法存在高偏差问题。</li>
<li>训练错误与交叉验证错误之间的差距：如果这个差距很大，说明算法存在高方差问题。</li>
</ul>
<h4 id="通过交叉验证选择合适的正则化参数λ">通过交叉验证选择合适的正则化参数λ</h4>
<p>较大的 Lambda
值会导致较高的bias（欠拟合），这时w偏小，拟合为直线。</p>
<p>较小的 Lambda
值会导致较高的variance（过拟合），相当于没有引入正则化。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747813206792.png" srcset="/img/loading.gif" lazyload alt="1747813206792">
<figcaption aria-hidden="true">1747813206792</figcaption>
</figure>
<p>选择过程：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747813236638.png" srcset="/img/loading.gif" lazyload alt="1747813236638">
<figcaption aria-hidden="true">1747813236638</figcaption>
</figure>
<h4 id="学习曲线">学习曲线</h4>
<p>学习曲线是一种图形工具，用于展示机器学习算法在不同训练集大小下的表现。</p>
<p>基本走势：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747815312013.png" srcset="/img/loading.gif" lazyload alt="1747815312013">
<figcaption aria-hidden="true">1747815312013</figcaption>
</figure>
<p>高偏差（欠拟合）：对比基准</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747815845281.png" srcset="/img/loading.gif" lazyload alt="1747815845281">
<figcaption aria-hidden="true">1747815845281</figcaption>
</figure>
<p>高方差（过拟合）：对比基准，增加m可以改善</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747815908367.png" srcset="/img/loading.gif" lazyload alt="1747815908367">
<figcaption aria-hidden="true">1747815908367</figcaption>
</figure>
<h4 id="欠拟合和过拟合矫正">欠拟合和过拟合矫正</h4>
<ul>
<li>欠拟合（高偏差）的解决方法：
<ol type="1">
<li><strong>添加更多特征</strong>：这有助于为模型提供更多信息，以捕捉数据中的结构。</li>
<li><strong>添加多项式特征</strong>：这可以使模型更复杂，更好地适应数据。</li>
<li><strong>减小正则化参数
Lambda</strong>：这将使模型更关注拟合训练数据，从而减少偏差。</li>
</ol></li>
<li>过拟合（高方差）的解决方法：
<ol type="1">
<li><strong>获取更多训练数据</strong>：这有助于减轻过拟合问题，提高模型的泛化能力。</li>
<li><strong>尝试减少特征数量</strong>：这将限制模型的复杂性，降低过拟合的风险。</li>
<li><strong>增大正则化参数
Lambda</strong>：这将迫使模型拟合更平滑、更简单的函数，以减少过拟合。</li>
</ol></li>
</ul>
<h4 id="处理偏差方差问题方法总结">处理偏差方差问题方法总结</h4>
<p>步骤如下：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747817587146.png" srcset="/img/loading.gif" lazyload alt="1747817587146">
<figcaption aria-hidden="true">1747817587146</figcaption>
</figure>
<p>一个更大的神经网络在性能上通常不会受到损害，只要经过适当的正则化。然而，训练一个更大的神经网络可能会增加计算成本。</p>
<p>代码如下:(Tensorflow)</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747817659838.png" srcset="/img/loading.gif" lazyload alt="1747817659838">
<figcaption aria-hidden="true">1747817659838</figcaption>
</figure>
<h2 id="机器学习开发">机器学习开发</h2>
<h3 id="误差分析">误差分析</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747899851050.png" srcset="/img/loading.gif" lazyload alt="1747899851050">
<figcaption aria-hidden="true">1747899851050</figcaption>
</figure>
<p>通过错误分析，你可以<strong>了解哪些类型的错误更为常见，并据此确定哪些方向更值得关注</strong>。例如，如果发现许多错误都是药品垃圾邮件，你可能会考虑收集更多药品垃圾邮件的数据，或者设计与药品名字相关的特征。同样，如果发现钓鱼邮件是常见的问题，你可以尝试检查邮件中的URL并提取相关特征，或者针对性地收集更多钓鱼邮件的数据。</p>
<h3 id="添加数据数据驱动">添加数据—数据驱动</h3>
<ul>
<li><strong>数据增强(Data
augmentation)</strong>：这是一种用于图像和音频数据的技术，通过对现有训练样本进行修改（如旋转、放大、缩小、改变对比度等），从而创造出新的训练样本。数据增强能显著提高训练集的大小，提升算法性能。</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747901048181.png" srcset="/img/loading.gif" lazyload alt="1747901048181">
<figcaption aria-hidden="true">1747901048181</figcaption>
</figure>
<ul>
<li><strong>数据合成 (Artificial data synthesis
)：与数据增强不同，数据合成是从头创建全新的训练样本，而非通过修改现有样本。</strong>例如，通过使用计算机中的不同字体、颜色和对比度合成文字图像，以生成大量用于光学字符识别任务的训练样本。</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747901106788.png" srcset="/img/loading.gif" lazyload alt="1747901106788">
<figcaption aria-hidden="true">1747901106788</figcaption>
</figure>
<ul>
<li>数据驱动方法：传统的机器学习研究方法主要集中在改进算法，而在许多情况下，现有的算法（如线性回归、逻辑回归、神经网络等）已经非常优秀。因此，更关注数据驱动方法，如收集更多特定类型的数据、使用数据增强和数据合成，可能是提升算法性能的更有效途径。</li>
</ul>
<h3 id="迁移学习">迁移学习</h3>
<p>在数据量有限且难以获取更多数据的情况下，迁移学习可以提升算法性能。其<strong>核心思想是利用来自与目标任务相关但不完全相同的任务的数据</strong>，在神经网络中对这些数据进行处理，以提高目标任务的算法性能。虽然并非所有任务都适用，但在适用的情况下，迁移学习可以非常强大。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1747902009472.png" srcset="/img/loading.gif" lazyload alt="1747902009472">
<figcaption aria-hidden="true">1747902009472</figcaption>
</figure>
<ul>
<li><strong>监督预训练（Supervised
Pre-training）</strong>：在一个大型数据集上训练神经网络，学习到通用的特征表示。<strong>迁移学习中预训练和微调的输入类型必须相同</strong>。例如，如果最终任务是计算机视觉任务，那么预训练阶段的神经网络也必须是在相同类型的图像输入上训练得到的。</li>
<li><strong>微调（Fine-tuning）</strong>：将预训练的神经网络参数应用到新任务中，并对网络参数进行进一步的优化。<strong>如果可以从大型数据集（如一百万张图像）上获取预训练的神经网络，那么有时只需用较小的数据集（如一千张图像，甚至更少）进行微调</strong>，就可以获得不错的结果。</li>
</ul>
<h2 id="倾斜数据集的错误指标">倾斜数据集的错误指标</h2>
<p>我们讨论了一个<strong>偏斜数据集</strong>的问题，即其中一个类（如罕见疾病）的样本数量远小于另一个类。在这种情况下，我们不能仅使用准确率（accuracy）作为评估指标，因为它可能会误导我们对分类器性能的判断。</p>
<h3 id="准确率和召回率">准确率和召回率</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748309464421.png" srcset="/img/loading.gif" lazyload alt="1748309464421">
<figcaption aria-hidden="true">1748309464421</figcaption>
</figure>
<ul>
<li><p><strong>精确率（Precision）</strong>：精确率是指<strong>在所有被分类为正类的样本中，实际上是正类的样本所占的比例。</strong>换句话说，当我们的分类器预测一个样本为正类时，我们有多大把握认为这个预测是正确的。精确率的计算公式是：</p>
<p>精确率 = True Positives / (True Positives + False Positives)</p></li>
<li><p><strong>召回率（Recall）</strong>：召回率是指在<strong>所有实际上是正类的样本中，被正确分类为正类的样本所占的比例。</strong>换句话说，我们的分类器能找到多少实际上是正类的样本。召回率的计算公式是：</p>
<p>召回率 = True Positives / (True Positives + False Negatives)</p></li>
</ul>
<p>权衡：</p>
<h3 id="阈值与召回率准确率">阈值与召回率、准确率</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748311358997.png" srcset="/img/loading.gif" lazyload alt="1748311358997">
<figcaption aria-hidden="true">1748311358997</figcaption>
</figure>
<p>阈值的调整：</p>
<ul>
<li>提高阈值：增加精确度（不那么容易判断为1，但判断为1的准确率会大大提高），降低召回率。</li>
<li>降低阈值：降低精确度，提高召回率。</li>
</ul>
<h3 id="召回率与准确率的综合度量f1score">召回率与准确率的综合度量：F1score</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748311399318.png" srcset="/img/loading.gif" lazyload alt="1748311399318">
<figcaption aria-hidden="true">1748311399318</figcaption>
</figure>
<p>F1分数（F1
Score）是一种将精确度和召回率结合起来的度量方法，它为不同算法提供了一个统一的评价指标。F1分数的计算公式为：</p>
<ul>
<li>F1 = 2 * (Precision * Recall) / (Precision + Recall)</li>
</ul>
<h2 id="决策树模型">决策树模型</h2>
<h3 id="决策树">决策树</h3>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748312803992.png" srcset="/img/loading.gif" lazyload alt="1748312803992">
<figcaption aria-hidden="true">1748312803992</figcaption>
</figure>
<p><strong>节点</strong>：决策树中的每个节点分为以下几种：</p>
<ul>
<li>根节点：位于树顶部的节点，从这里开始分类过程。</li>
<li>决策节点：根据特征值，根据特征值决定向左还是向右继续遍历树的节点。</li>
<li>叶子节点：树底部的节点，包含最终的预测结果。</li>
</ul>
<h3 id="决策树学习算法">决策树学习算法</h3>
<p>决策树学习算法的任务是在所有可能的决策树中选择一个表现良好的决策树，即在训练集上表现良好且能很好地泛化到新数据（如交叉验证集和测试集）的决策树。</p>
<h4 id="第一步如何选择分割特征">第一步：如何选择分割特征</h4>
<p>决策树的每个节点都需要选择一个特征来分割数据。<strong>我们希望选择能够使得分割后的数据尽量纯净（即同一类别的数据尽量聚集在一起）的特征后续，</strong>我们将学习如何<strong>通过计算熵来评估不纯度</strong>，并选择能够最大化纯度的特征进行分割。</p>
<h4 id="第二步什么时候停止分割">第二步：什么时候停止分割</h4>
<ul>
<li><strong>当节点中的数据完全纯净</strong>（即全部是同一类别）时，我们可以创建一个叶节点进行分类预测。</li>
<li><strong>当树的深度达到预设的最大深度时，</strong>可以停止分割。这样可以保证决策树不会过于复杂，从而降低过拟合的风险。</li>
<li><strong>当分割带来的纯度改善很小或低于一定阈值时</strong>，可以停止分割。这同样有助于保持决策树的简单性并减少过拟合风险。</li>
<li><strong>当节点中的数据数量低于一定阈值时</strong>，也可以停止分割。</li>
</ul>
<h4 id="衡量纯度">衡量纯度</h4>
<p>使用熵来计算不纯度，熵越大表示越不纯。</p>
<figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex">熵的公式如下：<br><br>H(p<span class="hljs-built_in">_</span>1) = -p<span class="hljs-built_in">_</span>1 * log2(p<span class="hljs-built_in">_</span>1) - p<span class="hljs-built_in">_</span>0 * log2(p<span class="hljs-built_in">_</span>0)<br></code></pre></td></tr></table></figure>
<p>其中，p_0 为样本中类别 0 的实例所占比例，且 p_0 = 1 -
p_1。在计算熵时，我们采用以 2 为底的对数，而不是以 e
为底。这样做是为了让曲线的峰值等于 1，使得结果更容易解释。</p>
<p>函数图像如下：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748315214399.png" srcset="/img/loading.gif" lazyload alt="1748315214399">
<figcaption aria-hidden="true">1748315214399</figcaption>
</figure>
<h4 id="选择拆分熵减信息增益information-gain">选择拆分—熵减=信息增益Information
Gain</h4>
<p>信息增益公式计算：</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748315853469.png" srcset="/img/loading.gif" lazyload alt="1748315853469">
<figcaption aria-hidden="true">1748315853469</figcaption>
</figure>
<p>其中：</p>
<p>p1,root 表示在该结点所有例子中正例的比例</p>
<p>p1,left 表示在左分支中正例的比例</p>
<p>w left 表示进入左分支的例子在其父节点的所有例子中占的比例</p>
<h3 id="处理具有多个离散值的特征独热编码">处理具有多个离散值的特征—独热编码</h3>
<p><strong>多值特征</strong>：在这个例子中，耳朵形状不再仅限于尖形和下垂，还可以是椭圆形。这意味着原始特征仍然是一个分类值特征，但它可以有三个可能的值而不仅仅是两个。</p>
<p><strong>独热编码</strong>：为了解决这个问题，可以使用独热编码。通过创建三个新特征来替换原始的耳朵形状特征：尖形耳朵、下垂耳朵和椭圆形耳朵。这样，<strong>每个样本的耳朵形状特征将被三个二值特征（0或1）替代。</strong></p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748396612417.png" srcset="/img/loading.gif" lazyload alt="1748396612417">
<figcaption aria-hidden="true">1748396612417</figcaption>
</figure>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748396630987.png" srcset="/img/loading.gif" lazyload alt="1748396630987">
<figcaption aria-hidden="true">1748396630987</figcaption>
</figure>
<h3 id="处理连续特征值">处理连续特征值</h3>
<p>处理连续值特征：与之前的方法类似，决策树学习算法需要考虑在不同特征上进行分割，包括体重特征。</p>
<p><strong>选择阈值：为了在连续值特征上进行分割，需要尝试多个阈值，然后选择能够产生最佳信息增益的阈值。</strong>例如，在这个例子中，算法可以尝试将阈值设置为8、9和13等不同的值，然后根据信息增益的计算结果选择最佳阈值。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748397117540.png" srcset="/img/loading.gif" lazyload alt="1748397117540">
<figcaption aria-hidden="true">1748397117540</figcaption>
</figure>
<h3 id="决策树-回归树预测值">决策树-&gt;回归树（预测值）</h3>
<h4 id="预测值">预测值</h4>
<p>对于决策树回归，<strong>预测是通过计算训练样本中相应叶节点的目标值（例如体重）的平均值来进行的</strong>。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748397615395.png" srcset="/img/loading.gif" lazyload alt="1748397615395">
<figcaption aria-hidden="true">1748397615395</figcaption>
</figure>
<h4 id="特征选择-方差减少">特征选择-方差减少</h4>
<p><strong>选择使方差减少最大的特征作为分裂特征。</strong></p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748397712017.png" srcset="/img/loading.gif" lazyload alt="1748397712017">
<figcaption aria-hidden="true">1748397712017</figcaption>
</figure>
<h3 id="多决策树树集">多决策树—树集</h3>
<ul>
<li><strong>单一决策树的缺点</strong>：单一决策树可能对数据中的小变化非常敏感，从而导致其鲁棒性较差。例如下面的例子，改变一个训练样本，树的根节点的分割特征就发生变化（根据信息增益公式选择）。</li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748398349046.png" srcset="/img/loading.gif" lazyload alt="1748398349046">
<figcaption aria-hidden="true">1748398349046</figcaption>
</figure>
<ul>
<li><p><strong>树集成（Tree Ensemble
）</strong>：为了解决单一决策树的鲁棒性问题，可以使用树集成方法。树集成是指构建多个决策树并将它们组合在一起，以提高模型的鲁棒性和预测准确性。</p>
<p><strong>投票策略</strong>：在树集成中，每个决策树都会对新的测试样本进行预测，然后根据这些预测进行投票。多数投票决定最终的预测结果。</p></li>
</ul>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748398451772.png" srcset="/img/loading.gif" lazyload alt="1748398451772">
<figcaption aria-hidden="true">1748398451772</figcaption>
</figure>
<h4 id="随机森林算法">随机森林算法</h4>
<p>创建树集的步骤：</p>
<ul>
<li>给定一个大小为M的训练集；</li>
<li>对于B=1到大写B（B表示要构建的树的数量，通常为100左右），执行以下操作：
<ul>
<li>使用<strong>有放回</strong>抽样创建一个大小为M的新训练集；</li>
<li>在<strong>新训练集</strong>上训练一个决策树；</li>
</ul></li>
<li>对于预测任务，让这些决策树进行投票以得到最终的预测结果。</li>
</ul>
<p>随机森林算法：</p>
<p>为了使树集成中的决策树更具多样性，我们可以在每个节点选择划分特征时进一步引入随机性。具体来说，<strong>在每个节点，我们可以从所有N个特征中随机选择一个大小为K（K小于N）的子集，然后从这个子集中选择具有最高信息增益的特征作为划分特征。</strong>当特征数量N较大时，一个典型的选择是令K等于N的平方根。</p>
<h4 id="xgboostextreme-gradient-boosting">XGBoost(extreme gradient
boosting)</h4>
<p>做法：</p>
<p>在有放回抽样创建新训练集时，<strong>专注于当前集成中表现不佳的示例</strong>。它通过修改各个样本的权重，在创建每个树时强调较难的示例。这个概念类似于刻意练习的思想，即学习者专注于自己最难掌握的领域。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748400504235.png" srcset="/img/loading.gif" lazyload alt="1748400504235">
<figcaption aria-hidden="true">1748400504235</figcaption>
</figure>
<p>实现：调库</p>
<p>XGBoost（极端梯度提升）是提升决策树的一种开源实现，速度快、效率高。<strong>XGBoost具有良好的默认分割标准和停止分割的标准。XGBoost的一个创新之处在于它具有内置的正则化功能，以防止过拟合</strong>。</p>
<figure>
<img src="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1748400612423.png" srcset="/img/loading.gif" lazyload alt="1748400612423">
<figcaption aria-hidden="true">1748400612423</figcaption>
</figure>
<h3 id="神经网络与决策树比较">神经网络与决策树比较</h3>
<ul>
<li>决策树和树集成：
<ul>
<li><strong>通常在表格数据（也称为结构化数据）上表现良好</strong>。如果你的数据集看起来像一个巨大的电子表格，那么决策树值得考虑。</li>
<li>不推荐在非结构化数据（如图像、视频、音频和文本）上使用决策树和树集成。</li>
<li>决策树和树集成可以非常快速地进行训练，这有助于更快地迭代和优化算法。</li>
<li>小型决策树可能具有可解释性，但大型决策树或树集成的可解释性可能被高估。</li>
<li>如果使用决策树或树集成，建议使用 XGBoost。</li>
</ul></li>
<li>神经网络：
<ul>
<li><strong>在所有类型的数据上表现良好，包括结构化数据、非结构化数据以及混合数据。</strong></li>
<li>与决策树相比，神经网络可能训练速度较慢。</li>
<li><strong>神经网络适用于迁移学习</strong>，对于只有少量数据的应用而言，这非常重要。</li>
<li>构建多个机器学习模型协同工作的系统时，将多个神经网络连接并训练可能比多个决策树更容易。</li>
</ul></li>
</ul>
<h2 id="无监督学习-1">无监督学习</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>1<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>1<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>1<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/machine-learning/" class="print-no-link">#machine learning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>机器学习笔记</div>
      <div>https://rain00star.github.io/2025/05/08/机器学习笔记/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>rainstar</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>May 8, 2025</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/05/13/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="java学习笔记">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">java学习笔记</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/04/14/BUUCTF%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/" title="">
                        <span class="hidden-mobile"></span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <div> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/js/duration.js"></script> </div> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>




  
<script src="/js/snowflake.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
